Prerequisites:
  1. Install WSL: can follow the guide here: https://learn.microsoft.com/en-us/windows/wsl/install
  2. Docker Desktop: https://docs.docker.com/desktop/install/mac-install/
  3. Signup a bigquery account and create a project, a dataset and a table
  4. A Google Cloud Function deployed for downloading data from website and can be triggered by HTTP, my cloud function called "OSP_scraping". Guide for deploying cloud function can be found here:https://github.com/chloeee-ye/data_pipeline_demo/blob/6f5ae82981fba96b0c4341bbcaa518fb693fb175/deploy%20a%20cloud%20function%20to%20scrape%20data%20from%20OSP
     * we can set it as public for easier access for our own study and experiment purpose through:
       1) click the cloud function you deployed, click the "PERMISSIONS" tab, click "GRANT ACCESS", input "allUsers" as new principles, choose "Cloud Function Invokers" as role, click save
       2) click "View in Cloud Run" on the top right corner, click the "SECURITY" tab, choose "Allow unauthenticated invocations"
  5. A bucket created in Google Cloud Storage，and set "asia-east2 (Hong Kong)" (my current region) as the region in order to run my cloud function 


Steps:
1. create a folder in a preferred location, mine called "OSP-airflow-docker" under ":c/Users/siton"
2. fetch docker-compose file using the terminal: curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.10.0/docker-compose.yaml', can refer to this link for the latest version: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html
3. create a Dickerfile under the folder "OSP-airflow-docker" with the below script

## start of the script ##
# Use a base image that includes Chrome
FROM selenium/standalone-chrome:latest

# Install necessary dependencies
USER root

# Install wget and unzip to download ChromeDriver
RUN apt-get update && \
    apt-get install -y wget unzip && \
    apt-get clean

# Set the ChromeDriver URL
ENV CHROMEDRIVER_URL="https://storage.googleapis.com/chrome-for-testing-public/129.0.6668.89/linux64/chromedriver-linux64.zip"

# Install ChromeDriver
RUN wget -N $CHROMEDRIVER_URL && \
    unzip chromedriver-linux64.zip -d /usr/local/bin/ && \
    chmod +x /usr/local/bin/chromedriver-linux64/chromedriver && \
    mv /usr/local/bin/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver && \
    rm -r /usr/local/bin/chromedriver-linux64 && \
    rm chromedriver-linux64.zip

FROM apache/airflow:2.10.2

USER root

# Install bash
RUN apt-get update && apt-get install -y bash && rm -rf /var/lib/apt/lists/*

# Switch back to the default user
USER airflow

## end of the script ##


4. create folders within the "airflow-docker" folder we just created:
   cd OSP-airflow-docker
   mkdir -p ./dags ./logs ./plugins
   echo -e "AIRFLOW_UID=$(id -u)\nAIRFLOW_GID=0" > .env
5. create a DAG py file within the "dags" folder we just created with the following scripts and remember to set dag id (mine is "bigquery_demo2") and input the value for 4 variables: 
   1) HTTP_CONN_ID: Airflow connection ID for HTTP, can leave it blank first
   2) BQ_CONN_ID: airflow connection with bigquery, can leave it blank first
   3) BQ_PROJECT 
   4) BQ_DATASET

## start of the script ##

import json
from datetime import timedelta, datetime

from airflow import DAG
from airflow.providers.http.sensors.http import HttpSensor
from airflow.providers.http.operators.http import SimpleHttpOperator


from airflow.contrib.operators.bigquery_operator import BigQueryOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.contrib.operators.bigquery_check_operator import BigQueryCheckOperator

from airflow.macros import ds_format, ds_add


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,    
    'start_date': datetime(2024, 8, 24),
    'email': ['airflow@airflow.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# Set Schedule: Run pipeline once a day. 
# Use cron to define exact time. Eg. 8:15am would be "15 08 * * *"
schedule_interval = "32 15 * * *"

# Define DAG: Set ID and assign default args and schedule interval
with DAG(
    'OSPDataPipeline',
    start_date=datetime(2024, 8, 24),
    schedule_interval=schedule_interval,
    default_args=default_args,
    max_active_runs=1,
    template_searchpath="/opt/airflow/dags/sql",
    catchup=True,
) as dag:

    # Config variables
    HTTP_CONN_ID = "puton_conn_id"
    BQ_CONN_ID = "my_gcp_conn"
    BQ_PROJECT = "pigpig-project-432914"
    BQ_DATASET = "pigpigdata"

    ## Task 1: trigger the cloud function to fetch crime data  
    trigger_cloud_function = SimpleHttpOperator(
        task_id='trigger_cloud_function',
        http_conn_id='puton_conn_id', 
        endpoint='<cloud-function-endpoint>/scraping-function-001/?date={{ ds }}',
        headers={"Content-Type": "application/json"},
        method='POST',
        response_check=lambda response: response.status_code == 200
    )

    # ## Task 2: load the downloaded csv into the BigQuery table
    load_csv_to_bigquery = GCSToBigQueryOperator(
            task_id='load_csv_to_bigquery',
            gcp_conn_id='my_gcp_conn',
            bucket='puton_bucket_002',  # Replace with your GCS bucket name
            source_objects=['tmp/public_estate_geo.csv'],  # Replace with the path to your CSV file in the bucket
            destination_project_dataset_table='{0}.{1}.osp_public_estate_geo_raw'.format(
                BQ_PROJECT, BQ_DATASET
            ),  
            create_disposition='CREATE_IF_NEEDED',  # Create the table if it doesn't exist
            write_disposition='WRITE_TRUNCATE',  # Overwrite the table if it already exists
            skip_leading_rows=1,  # Skip the header row
            source_format='CSV',
            autodetect=True,  # Automatically detect the schema from the CSV file
        )

    # ## Task 3: fetch and clean the raw data from the london_recent_crime_data_raw table and insert the data to the staging dataset
    cleaning_OSP_data_insert = BigQueryOperator(
            task_id='cleaning_OSP_data_insert',
            sql='''
                WITH cleaned_coordinates AS (
                SELECT 
                    string_field_0 AS name,
                    SPLIT(REPLACE(REPLACE(REPLACE(TRIM(string_field_1), '[', ''), ']', ''), "'", ''), ' ') AS coord_array
                FROM `pigpig-project-432914.pigpigdata.osp_public_estate_geo_raw`
                )

                SELECT 
                name,
                ST_GEOGFROMTEXT(CONCAT('POLYGON(( ',
                    STRING_AGG(CONCAT(SPLIT(coord, ',')[OFFSET(1)], ' ', SPLIT(coord, ',')[OFFSET(0)]), ', '), 
                    ' ))')) AS polygon
                FROM cleaned_coordinates,
                UNNEST(coord_array) AS coord 
                GROUP BY name
            ''',
            destination_project_dataset_table='{0}.{1}.osp_public_estate_polygon'.format(
                BQ_PROJECT, BQ_DATASET
            ),     
            write_disposition='WRITE_TRUNCATE',
            create_disposition='CREATE_IF_NEEDED',
            allow_large_results=True,
            use_legacy_sql=False,
            gcp_conn_id='my_gcp_conn',
            dag=dag
        )



    # #Setting up Dependencies
     trigger_cloud_function >> load_csv_to_bigquery >> cleaning_OSP_data_insert

## end of the script ##

6. create a service account from GCP to allow access from our local machine to the GCP: 
  open the GCP "Naviagtion menu", click "IAM and Admin", click "Service accounts", 
  click the "CREATE SERVICE ACCOUNT" button, input Service account name, click "CREATE AND CONTINUE", select a proper role and click "CONTINUE", click "DONE"

7. generate a JSON key for the service account we just created: under the "Action" pannel on the right hand side, click "Manage details", click "KEYS", click "ADD KEY", click "Create new key", a new JSON key which will be download automatically. 
   Create a folder called "support" within folder "dags" and create a folder called "keys" within the folder "support" and move the key file to the "keys" folder

8. run command to initialise the environment: docker-compose up airflow-init

9. run command: docker-compose up, once it’s finished will see the airflow-webserver keeps updating the health status and display the message in the terminal.

10. Access the Airflow UI by using your web brower entering the following address: http://localhost:8080/home, use “airflow” as id and password. 

11. create a connection with google clound: click "Admin", click "Connections", click "Add a new record", configure the connection as follows and then click "Save"
   Connection Id: my_gcp_conn (can be any name)
   Connection Type: Google Cloud
   Project Id: bigquery project id, mine is "pigpig-project-432914"
   Keyfile Path: starts with /opt/airflow/dags/, mine is "/opt/airflow/dags/support/keys/pigpig-project-432914-32f163bcf106.json"
   Scopes (comma separated): https://www.googleapis.com/auth/cloud-platform

12. create Airflow connection ID for HTTP: click "Admin", click "Connections", click "Add a new record", configure the connection as follows and then click "Save"
   Connection Id: puton_conn_id (can be any name)
   Connection Type: HTTP
   Host: cloud function url, mine is "https://us-central1-pigpig-project-432914.cloudfunctions.net/OSP_scraping"

13. put the connection id , mine is "my_gcp_conn" as the value for the variable "BQ_CONN_ID" and HTTP connection id, mine is "puton_conn_id" as the value for the variable "HTTP_CONN_ID" in the dag file

14. test run: docker-compose -f docker-compose.yaml run --rm airflow-webserver airflow tasks test trigger_cloud_function 2024-08-18
              docker-compose -f docker-compose.yaml run --rm airflow-webserver airflow tasks test (DAG id) (task_id) (execute date)

15. Go to airflow UI and toggle the dag we just created, it will run the task according to the schedule we set up

  
